%==========================================================================
\subsection{Distribuição $\Gamma(w;k,\theta)$}
\begin{frame}
  \frametitle{Distribuição $\Gamma(w;k,\theta)$}
  
  A Distribuição Gamma é caracterizada por dois valores, denominados
  \textit{shape} ($k$) e \textit{scale} ($\theta$). Ela possui a
  seguinte função densidade probabilidade:

  \begin{displaymath}
    f(w;k,\theta) = \frac{w^{k-1}e^{-\frac{w}{\theta}}}{\theta^k\Gamma(w)}
  \end{displaymath}
  
  para, $ w > 0 $ e $ k,\theta > 0 $ 
\end{frame}

%==========================================================================
\begin{frame}
  \frametitle{Distribuição $\Gamma(w;k,\theta)$}

  Esperança da distribuição $\Gamma(w;k,\theta)$:

  \begin{equation}
    \label{eq:G_esp}
     E[W] = k\theta
  \end{equation}\pause
  
  A variância da distribuição $\Gamma(w;k,\theta)$:
  
  \begin{equation}
    \label{eq:G_var}
      Var[W] = k\theta^2
  \end{equation}

  Em nosso estudo usaremos sempre $\theta = 1$ para simplicidade dos cálculos.
\end{frame}

% ====================
\subsubsection{Estimadores}
% ====================
\begin{frame}
  \frametitle{Estimadores da Distribuição $\Gamma(w;k,1)$}
  
  Para estimar pela Máxima Verossimilhança é preciso encontrar o
  máximo da função log-verossimilhança de $\Gamma(w;k,1)$

  \small \begin{align*}
%    \label{eq:Gamma_MV}
    log(p(W|k,1)) &= n(k-1)\overline{log(x)} - n log(\Gamma(k)) - n k
    log(\overline{x}) + \\
    & n k log(a) - n k
    \intertext{Que podemos resolver numericamente iterando sobre k em:}
%    log(p(W|k,1)) &\approx c_0 + c_1a + c_2log(a)
    \frac{1}{k} &= \frac{1}{k_0} + \frac{\overline{log(x)} -
      log(\overline{x}) + log(k_0) - \psi(k_0)}{k_0^2 (\frac{1}{k_0} -
      \psi'(k_0))}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Estimadores da Distribuição $\Gamma(w;k,1)$}
  Quando $k \approx k_0$ então encontramos o estimador $\hat{k}$  

  \small \begin{align*}
%    \label{eq:Gamma_MV}
    \intertext{Como k inicial podemos usar a seguinte aproximação}
    \hat{k} &= \frac{0.5}{log(\overline{x}) - \overline{log(x)}}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Estimadores da Distribuição $\Gamma(w;k,1)$}
  
  Estimador pelo primeiro momento:
  
    \begin{align*}
%      \label{eq:G_m_p2}
      E[W] &= k\theta \\
      \hat{k}_1 &= \frac{1}{n} \sum_{i=1}^nw_i 
    \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Estimadores da Distribuição $\Gamma(w;k,1)$}
  
  Estimador pelo segundo momento central:
  
    \begin{align*}
%      \label{eq:G_m_p3}
      Var[W] &= k\theta^2 \\
      \hat{k}_2^0 &= Var(w)  
    \end{align*}
\end{frame}


%===================
\subsubsection{Gráficos}
%=========================================================================================================================================================
\begin{frame}
\frametitle{Gráficos da distribuição $U(x; 0,\theta)$}

\end{frame}
%=========================================================================================================================================================



